{"cells":[{"cell_type":"markdown","metadata":{},"source":["**Linear regression**\n","\n","Her er en start på nogen linæer regression;\n","Der er ikke brugt regulization; og der bruges et simpelt K-fold lige nu"]},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[],"source":["# packages\n","import numpy as np\n","import pandas as pd\n","import sklearn.linear_model as lm\n","from sklearn import model_selection\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["# loading data\n","filename = 'glass.csv'\n","df = pd.read_csv(filename)"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>RI</th>\n","      <th>Na</th>\n","      <th>Mg</th>\n","      <th>Al</th>\n","      <th>Si</th>\n","      <th>K</th>\n","      <th>Ca</th>\n","      <th>Ba</th>\n","      <th>Fe</th>\n","      <th>Type</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-1.719943</td>\n","      <td>0.870826</td>\n","      <td>0.284287</td>\n","      <td>1.251704</td>\n","      <td>-0.690822</td>\n","      <td>-1.124446</td>\n","      <td>-0.670134</td>\n","      <td>-0.145425</td>\n","      <td>-0.352051</td>\n","      <td>-0.585079</td>\n","      <td>-0.846290</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-1.703794</td>\n","      <td>-0.248750</td>\n","      <td>0.590433</td>\n","      <td>0.634680</td>\n","      <td>-0.170061</td>\n","      <td>0.102080</td>\n","      <td>-0.026152</td>\n","      <td>-0.791877</td>\n","      <td>-0.352051</td>\n","      <td>-0.585079</td>\n","      <td>-0.846290</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-1.687644</td>\n","      <td>-0.719631</td>\n","      <td>0.149582</td>\n","      <td>0.600016</td>\n","      <td>0.190465</td>\n","      <td>0.437760</td>\n","      <td>-0.164148</td>\n","      <td>-0.827010</td>\n","      <td>-0.352051</td>\n","      <td>-0.585079</td>\n","      <td>-0.846290</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-1.671494</td>\n","      <td>-0.232286</td>\n","      <td>-0.242285</td>\n","      <td>0.697076</td>\n","      <td>-0.310266</td>\n","      <td>-0.052850</td>\n","      <td>0.111844</td>\n","      <td>-0.517838</td>\n","      <td>-0.352051</td>\n","      <td>-0.585079</td>\n","      <td>-0.846290</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-1.655344</td>\n","      <td>-0.311315</td>\n","      <td>-0.168810</td>\n","      <td>0.648546</td>\n","      <td>-0.410413</td>\n","      <td>0.553957</td>\n","      <td>0.081178</td>\n","      <td>-0.623237</td>\n","      <td>-0.352051</td>\n","      <td>-0.585079</td>\n","      <td>-0.846290</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>209</th>\n","      <td>1.655344</td>\n","      <td>-0.703166</td>\n","      <td>0.896579</td>\n","      <td>-1.861147</td>\n","      <td>2.874386</td>\n","      <td>-0.052850</td>\n","      <td>-0.639468</td>\n","      <td>0.156721</td>\n","      <td>1.779805</td>\n","      <td>-0.585079</td>\n","      <td>2.005775</td>\n","    </tr>\n","    <tr>\n","      <th>210</th>\n","      <td>1.671494</td>\n","      <td>-0.499008</td>\n","      <td>1.851755</td>\n","      <td>-1.861147</td>\n","      <td>1.091782</td>\n","      <td>0.528136</td>\n","      <td>-0.762132</td>\n","      <td>-0.391358</td>\n","      <td>2.845733</td>\n","      <td>-0.585079</td>\n","      <td>2.005775</td>\n","    </tr>\n","    <tr>\n","      <th>211</th>\n","      <td>1.687644</td>\n","      <td>0.752282</td>\n","      <td>1.165987</td>\n","      <td>-1.861147</td>\n","      <td>1.151869</td>\n","      <td>0.992924</td>\n","      <td>-0.762132</td>\n","      <td>-0.363251</td>\n","      <td>2.946292</td>\n","      <td>-0.585079</td>\n","      <td>2.005775</td>\n","    </tr>\n","    <tr>\n","      <th>212</th>\n","      <td>1.703794</td>\n","      <td>-0.610966</td>\n","      <td>1.190479</td>\n","      <td>-1.861147</td>\n","      <td>0.991635</td>\n","      <td>1.238229</td>\n","      <td>-0.762132</td>\n","      <td>-0.335145</td>\n","      <td>2.805509</td>\n","      <td>-0.585079</td>\n","      <td>2.005775</td>\n","    </tr>\n","    <tr>\n","      <th>213</th>\n","      <td>1.719943</td>\n","      <td>-0.413394</td>\n","      <td>1.006792</td>\n","      <td>-1.861147</td>\n","      <td>1.272045</td>\n","      <td>0.915460</td>\n","      <td>-0.762132</td>\n","      <td>-0.236772</td>\n","      <td>3.006628</td>\n","      <td>-0.585079</td>\n","      <td>2.005775</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>214 rows × 11 columns</p>\n","</div>"],"text/plain":["           ID        RI        Na        Mg        Al        Si         K  \\\n","0   -1.719943  0.870826  0.284287  1.251704 -0.690822 -1.124446 -0.670134   \n","1   -1.703794 -0.248750  0.590433  0.634680 -0.170061  0.102080 -0.026152   \n","2   -1.687644 -0.719631  0.149582  0.600016  0.190465  0.437760 -0.164148   \n","3   -1.671494 -0.232286 -0.242285  0.697076 -0.310266 -0.052850  0.111844   \n","4   -1.655344 -0.311315 -0.168810  0.648546 -0.410413  0.553957  0.081178   \n","..        ...       ...       ...       ...       ...       ...       ...   \n","209  1.655344 -0.703166  0.896579 -1.861147  2.874386 -0.052850 -0.639468   \n","210  1.671494 -0.499008  1.851755 -1.861147  1.091782  0.528136 -0.762132   \n","211  1.687644  0.752282  1.165987 -1.861147  1.151869  0.992924 -0.762132   \n","212  1.703794 -0.610966  1.190479 -1.861147  0.991635  1.238229 -0.762132   \n","213  1.719943 -0.413394  1.006792 -1.861147  1.272045  0.915460 -0.762132   \n","\n","           Ca        Ba        Fe      Type  \n","0   -0.145425 -0.352051 -0.585079 -0.846290  \n","1   -0.791877 -0.352051 -0.585079 -0.846290  \n","2   -0.827010 -0.352051 -0.585079 -0.846290  \n","3   -0.517838 -0.352051 -0.585079 -0.846290  \n","4   -0.623237 -0.352051 -0.585079 -0.846290  \n","..        ...       ...       ...       ...  \n","209  0.156721  1.779805 -0.585079  2.005775  \n","210 -0.391358  2.845733 -0.585079  2.005775  \n","211 -0.363251  2.946292 -0.585079  2.005775  \n","212 -0.335145  2.805509 -0.585079  2.005775  \n","213 -0.236772  3.006628 -0.585079  2.005775  \n","\n","[214 rows x 11 columns]"]},"execution_count":70,"metadata":{},"output_type":"execute_result"}],"source":["# standardizing\n","df_stand = (df - df.mean(axis = 0)) / df.std(axis = 0)\n","df_stand"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[],"source":["# defining x and y for the linear regression\n","X = np.array(df_stand.iloc[:,2:10])\n","y = np.array(df_stand.iloc[:,1])"]},{"cell_type":"code","execution_count":92,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[ 0.35972783  0.72256044 -0.7758665   0.70327487  0.48813391  0.13316859\n"," -0.0533471   0.23764535 -0.10185475]\n"]}],"source":["# K-fold\n","\n","K = 5\n","CV = model_selection.KFold(n_splits=K,shuffle=True)\n","\n","error_test = np.zeros(K)\n","error_train = np.zeros(K)\n","\n","for i, (train_index, test_index) in enumerate(CV.split(X)):\n","    X_train, X_test = X[train_index, :], X[test_index, :]\n","    y_train, y_test = y[train_index], y[test_index]\n","    \n","    lin = lm.LinearRegression(fit_intercept=True).fit(X_train, y_train)\n","    error_train[i] = 1/X_train.shape[0] * np.sum(np.square(y_train - lin.predict(X_train)))\n","    error_test[i] = 1/X_test.shape[0] * np.sum(np.square(y_test - lin.predict(X_test)))\n","\n","\n","    if i == K-1:\n","        print(lin.coef_)\n"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[{"data":{"text/plain":["array([0.25072502, 0.10033162, 0.07460681, 0.04570369, 0.10891404])"]},"execution_count":68,"metadata":{},"output_type":"execute_result"}],"source":["error_test"]},{"cell_type":"markdown","metadata":{},"source":["**Artificial Neural Network**\n","\n","Har skrevet træningsloopet ind fordi jeg ville lære det; men tænker bare det kan fjernes. Her er en start"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import torch\n","import numpy as np\n","import pandas as pd\n","import sklearn.linear_model as lm\n","from sklearn import model_selection\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>RI</th>\n","      <th>Na</th>\n","      <th>Mg</th>\n","      <th>Al</th>\n","      <th>Si</th>\n","      <th>K</th>\n","      <th>Ca</th>\n","      <th>Ba</th>\n","      <th>Fe</th>\n","      <th>Type</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-1.719943</td>\n","      <td>0.870826</td>\n","      <td>0.284287</td>\n","      <td>1.251704</td>\n","      <td>-0.690822</td>\n","      <td>-1.124446</td>\n","      <td>-0.670134</td>\n","      <td>-0.145425</td>\n","      <td>-0.352051</td>\n","      <td>-0.585079</td>\n","      <td>-0.846290</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-1.703794</td>\n","      <td>-0.248750</td>\n","      <td>0.590433</td>\n","      <td>0.634680</td>\n","      <td>-0.170061</td>\n","      <td>0.102080</td>\n","      <td>-0.026152</td>\n","      <td>-0.791877</td>\n","      <td>-0.352051</td>\n","      <td>-0.585079</td>\n","      <td>-0.846290</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-1.687644</td>\n","      <td>-0.719631</td>\n","      <td>0.149582</td>\n","      <td>0.600016</td>\n","      <td>0.190465</td>\n","      <td>0.437760</td>\n","      <td>-0.164148</td>\n","      <td>-0.827010</td>\n","      <td>-0.352051</td>\n","      <td>-0.585079</td>\n","      <td>-0.846290</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-1.671494</td>\n","      <td>-0.232286</td>\n","      <td>-0.242285</td>\n","      <td>0.697076</td>\n","      <td>-0.310266</td>\n","      <td>-0.052850</td>\n","      <td>0.111844</td>\n","      <td>-0.517838</td>\n","      <td>-0.352051</td>\n","      <td>-0.585079</td>\n","      <td>-0.846290</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-1.655344</td>\n","      <td>-0.311315</td>\n","      <td>-0.168810</td>\n","      <td>0.648546</td>\n","      <td>-0.410413</td>\n","      <td>0.553957</td>\n","      <td>0.081178</td>\n","      <td>-0.623237</td>\n","      <td>-0.352051</td>\n","      <td>-0.585079</td>\n","      <td>-0.846290</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>209</th>\n","      <td>1.655344</td>\n","      <td>-0.703166</td>\n","      <td>0.896579</td>\n","      <td>-1.861147</td>\n","      <td>2.874386</td>\n","      <td>-0.052850</td>\n","      <td>-0.639468</td>\n","      <td>0.156721</td>\n","      <td>1.779805</td>\n","      <td>-0.585079</td>\n","      <td>2.005775</td>\n","    </tr>\n","    <tr>\n","      <th>210</th>\n","      <td>1.671494</td>\n","      <td>-0.499008</td>\n","      <td>1.851755</td>\n","      <td>-1.861147</td>\n","      <td>1.091782</td>\n","      <td>0.528136</td>\n","      <td>-0.762132</td>\n","      <td>-0.391358</td>\n","      <td>2.845733</td>\n","      <td>-0.585079</td>\n","      <td>2.005775</td>\n","    </tr>\n","    <tr>\n","      <th>211</th>\n","      <td>1.687644</td>\n","      <td>0.752282</td>\n","      <td>1.165987</td>\n","      <td>-1.861147</td>\n","      <td>1.151869</td>\n","      <td>0.992924</td>\n","      <td>-0.762132</td>\n","      <td>-0.363251</td>\n","      <td>2.946292</td>\n","      <td>-0.585079</td>\n","      <td>2.005775</td>\n","    </tr>\n","    <tr>\n","      <th>212</th>\n","      <td>1.703794</td>\n","      <td>-0.610966</td>\n","      <td>1.190479</td>\n","      <td>-1.861147</td>\n","      <td>0.991635</td>\n","      <td>1.238229</td>\n","      <td>-0.762132</td>\n","      <td>-0.335145</td>\n","      <td>2.805509</td>\n","      <td>-0.585079</td>\n","      <td>2.005775</td>\n","    </tr>\n","    <tr>\n","      <th>213</th>\n","      <td>1.719943</td>\n","      <td>-0.413394</td>\n","      <td>1.006792</td>\n","      <td>-1.861147</td>\n","      <td>1.272045</td>\n","      <td>0.915460</td>\n","      <td>-0.762132</td>\n","      <td>-0.236772</td>\n","      <td>3.006628</td>\n","      <td>-0.585079</td>\n","      <td>2.005775</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>214 rows × 11 columns</p>\n","</div>"],"text/plain":["           ID        RI        Na        Mg        Al        Si         K  \\\n","0   -1.719943  0.870826  0.284287  1.251704 -0.690822 -1.124446 -0.670134   \n","1   -1.703794 -0.248750  0.590433  0.634680 -0.170061  0.102080 -0.026152   \n","2   -1.687644 -0.719631  0.149582  0.600016  0.190465  0.437760 -0.164148   \n","3   -1.671494 -0.232286 -0.242285  0.697076 -0.310266 -0.052850  0.111844   \n","4   -1.655344 -0.311315 -0.168810  0.648546 -0.410413  0.553957  0.081178   \n","..        ...       ...       ...       ...       ...       ...       ...   \n","209  1.655344 -0.703166  0.896579 -1.861147  2.874386 -0.052850 -0.639468   \n","210  1.671494 -0.499008  1.851755 -1.861147  1.091782  0.528136 -0.762132   \n","211  1.687644  0.752282  1.165987 -1.861147  1.151869  0.992924 -0.762132   \n","212  1.703794 -0.610966  1.190479 -1.861147  0.991635  1.238229 -0.762132   \n","213  1.719943 -0.413394  1.006792 -1.861147  1.272045  0.915460 -0.762132   \n","\n","           Ca        Ba        Fe      Type  \n","0   -0.145425 -0.352051 -0.585079 -0.846290  \n","1   -0.791877 -0.352051 -0.585079 -0.846290  \n","2   -0.827010 -0.352051 -0.585079 -0.846290  \n","3   -0.517838 -0.352051 -0.585079 -0.846290  \n","4   -0.623237 -0.352051 -0.585079 -0.846290  \n","..        ...       ...       ...       ...  \n","209  0.156721  1.779805 -0.585079  2.005775  \n","210 -0.391358  2.845733 -0.585079  2.005775  \n","211 -0.363251  2.946292 -0.585079  2.005775  \n","212 -0.335145  2.805509 -0.585079  2.005775  \n","213 -0.236772  3.006628 -0.585079  2.005775  \n","\n","[214 rows x 11 columns]"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# loading data\n","filename = 'glass.csv'\n","df = pd.read_csv(filename)\n","\n","# standardizing\n","df_stand = (df - df.mean(axis = 0)) / df.std(axis = 0)\n","df_stand"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([214, 9])\n","torch.Size([214])\n"]}],"source":["X = torch.tensor(np.array(df_stand.iloc[:,1:10]), dtype=torch.float)\n","y = torch.tensor(np.array(df.iloc[:,10]), dtype=torch.long)\n","N, M =  X.shape\n","print(X.shape)\n","print(y.shape)\n","y[y>3] = y[y>3] - 1\n","y = y - 1"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Training model of type:\n","Sequential(\n","  (0): Linear(in_features=9, out_features=3, bias=True)\n","  (1): ReLU()\n","  (2): Linear(in_features=3, out_features=6, bias=True)\n","  (3): Softmax(dim=1)\n",")\n","\n"]},{"name":"stderr","output_type":"stream","text":["/var/folders/k7/4m0jgf1s5rggf6q996sfswf00000gn/T/ipykernel_13379/3154283903.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  X_train, X_test = torch.tensor(X[train_index, :], dtype=torch.float), torch.tensor(X[test_index, :], dtype=torch.float)\n","/var/folders/k7/4m0jgf1s5rggf6q996sfswf00000gn/T/ipykernel_13379/3154283903.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_train, y_test = torch.tensor(y[train_index]) , torch.tensor(y[test_index])\n"]},{"name":"stdout","output_type":"stream","text":["\t\t1000\t1.4212689\t6.340567e-05\n","\t\t2000\t1.3761117\t1.8711216e-05\n","\t\t3000\t1.3526458\t1.5775104e-05\n","\t\t4000\t1.3347645\t9.02034e-06\n","\t\t5000\t1.3265259\t4.3135387e-06\n","\t\t6000\t1.3208735\t5.3247422e-06\n","\t\t7000\t1.2938268\t6.4495466e-06\n","\t\tFinal Loss:\n","\t\t7815\t1.2894652\t6.4714e-07\n","\t\t1000\t1.4707824\t0.00010592324\n","\t\t2000\t1.3833723\t4.8340695e-05\n","\t\t3000\t1.3313228\t3.948646e-05\n","\t\t4000\t1.3068758\t1.0216199e-05\n","\t\t5000\t1.2972841\t5.6053455e-06\n","\t\t6000\t1.291452\t3.6922424e-06\n","\t\t7000\t1.2876145\t2.8700185e-06\n","\t\t8000\t1.2839813\t7.427423e-06\n","\t\t9000\t1.2646439\t4.1475605e-06\n","\t\t10000\t1.2608564\t2.647289e-06\n","\t\tFinal Loss:\n","\t\t10000\t1.2608564\t2.647289e-06\n","\t\t1000\t1.4194802\t0.00010034716\n","\t\t2000\t1.3581669\t3.335232e-05\n","\t\t3000\t1.303974\t1.9472078e-05\n","\t\t4000\t1.2641646\t2.4799987e-05\n","\t\t5000\t1.250227\t5.7209745e-06\n","\t\t6000\t1.245544\t2.4884177e-06\n","\t\t7000\t1.2430524\t1.2467044e-06\n","\t\tFinal Loss:\n","\t\t7206\t1.2427267\t9.59255e-07\n","\t\t1000\t1.4475276\t7.930034e-05\n","\t\t2000\t1.3872983\t2.8527646e-05\n","\t\t3000\t1.3350381\t2.2947723e-05\n","\t\t4000\t1.3176548\t7.689959e-06\n","\t\t5000\t1.2753277\t3.1219162e-05\n","\t\tFinal Loss:\n","\t\t5922\t1.2618822\t5.6681625e-07\n","\t\t1000\t1.5146879\t9.600748e-05\n","\t\t2000\t1.43948\t4.8857954e-05\n","\t\t3000\t1.379138\t3.3277345e-05\n","\t\t4000\t1.3485498\t1.2817565e-05\n","\t\t5000\t1.327613\t9.7872535e-06\n","\t\t6000\t1.3132819\t5.8093774e-06\n","\t\t7000\t1.3054091\t4.2919974e-06\n","\t\t8000\t1.3008704\t3.024048e-06\n","\t\tFinal Loss:\n","\t\t8412\t1.298623\t6.425764e-07\n","\t\t1000\t1.4986532\t4.5576795e-05\n","\t\t2000\t1.4464521\t2.637209e-05\n","\t\t3000\t1.4034208\t5.971062e-05\n","\t\t4000\t1.3804291\t1.23488535e-05\n","\t\t5000\t1.3669627\t6.8021327e-06\n","\t\tFinal Loss:\n","\t\t5469\t1.362407\t7.8749065e-07\n","\t\t1000\t1.3958337\t6.2596955e-05\n","\t\t2000\t1.3544174\t1.6018505e-05\n","\t\t3000\t1.3369486\t8.827276e-06\n","\t\t4000\t1.3260432\t8.270593e-06\n","\t\tFinal Loss:\n","\t\t4262\t1.3236575\t9.0060445e-07\n","\t\t1000\t1.4265964\t6.876682e-05\n","\t\t2000\t1.3569682\t3.109784e-05\n","\t\t3000\t1.3297924\t1.1922648e-05\n","\t\t4000\t1.3178663\t6.3319e-06\n","\t\t5000\t1.3104751\t4.8211996e-06\n","\t\t6000\t1.303897\t4.20555e-06\n","\t\t7000\t1.2990675\t3.578833e-06\n","\t\t8000\t1.2947695\t2.9462278e-06\n","\t\t9000\t1.2918687\t1.753253e-06\n","\t\tFinal Loss:\n","\t\t9562\t1.2908039\t9.235266e-07\n","\t\t1000\t1.4874566\t0.000240131\n","\t\t2000\t1.3272995\t3.870808e-05\n","\t\t3000\t1.286402\t2.0386713e-05\n","\t\t4000\t1.2673517\t1.1475398e-05\n","\t\t5000\t1.2554935\t7.8808e-06\n","\t\t6000\t1.245885\t7.941578e-06\n","\t\t7000\t1.2393365\t3.847505e-06\n","\t\t8000\t1.2353045\t2.3160414e-06\n","\t\t9000\t1.2331542\t1.3533813e-06\n","\t\tFinal Loss:\n","\t\t9256\t1.2327514\t9.670172e-07\n","\t\t1000\t1.4613041\t0.00010718113\n","\t\t2000\t1.3910553\t3.7790956e-05\n","\t\t3000\t1.3366758\t3.335348e-05\n","\t\t4000\t1.3054285\t1.689357e-05\n","\t\t5000\t1.2851063\t9.554416e-06\n","\t\t6000\t1.2749845\t6.731855e-06\n","\t\t7000\t1.2678125\t4.983435e-06\n","\t\t8000\t1.2621068\t4.155898e-06\n","\t\t9000\t1.2571182\t3.6034294e-06\n","\t\t10000\t1.253554\t2.282324e-06\n","\t\tFinal Loss:\n","\t\t10000\t1.253554\t2.282324e-06\n","\t\t1000\t1.4615421\t0.00010015062\n","\t\t2000\t1.3785185\t2.0580956e-05\n","\t\t3000\t1.3474507\t2.441719e-05\n","\t\t4000\t1.3184962\t2.0071298e-05\n","\t\t5000\t1.2906545\t1.5147374e-05\n","\t\t6000\t1.2749985\t1.3369977e-05\n","\t\t7000\t1.2609112\t6.2397444e-06\n","\t\t8000\t1.2535992\t4.46938e-06\n","\t\t9000\t1.2486756\t4.868874e-06\n","\t\t10000\t1.2440869\t2.9704333e-06\n","\t\tFinal Loss:\n","\t\t10000\t1.2440869\t2.9704333e-06\n","\t\t1000\t1.4560449\t0.00010912345\n","\t\t2000\t1.3702873\t3.1578493e-05\n","\t\t3000\t1.3371911\t1.925582e-05\n","\t\t4000\t1.3126872\t1.6618535e-05\n","\t\t5000\t1.2954364\t1.0030351e-05\n","\t\t6000\t1.2849694\t5.6590648e-06\n","\t\t7000\t1.2781157\t4.1971125e-06\n","\t\t8000\t1.273361\t3.2766134e-06\n","\t\t9000\t1.2696736\t3.098351e-06\n","\t\t10000\t1.2667639\t2.164419e-06\n","\t\tFinal Loss:\n","\t\t10000\t1.2667639\t2.164419e-06\n","\t\t1000\t1.4311365\t4.347911e-05\n","\t\t2000\t1.3962098\t2.3906008e-05\n","\t\t3000\t1.3667084\t1.7357204e-05\n","\t\t4000\t1.3387401\t1.6740345e-05\n","\t\t5000\t1.3205956\t1.1012738e-05\n","\t\t6000\t1.3099115\t6.006334e-06\n","\t\t7000\t1.3030492\t5.489062e-06\n","\t\t8000\t1.2956179\t4.600459e-06\n","\t\t9000\t1.2904935\t3.2331134e-06\n","\t\tFinal Loss:\n","\t\t9823\t1.2875115\t9.258883e-07\n","\t\t1000\t1.4769716\t0.00019609163\n","\t\t2000\t1.3906319\t2.6487738e-05\n","\t\t3000\t1.3590866\t2.1226046e-05\n","\t\t4000\t1.3376224\t1.1407266e-05\n","\t\t5000\t1.3252861\t8.455214e-06\n","\t\t6000\t1.3162479\t7.335914e-06\n","\t\t7000\t1.3077155\t5.560633e-06\n","\t\t8000\t1.3022546\t3.2954542e-06\n","\t\t9000\t1.2985449\t2.2032482e-06\n","\t\t10000\t1.2962286\t1.4714578e-06\n","\t\tFinal Loss:\n","\t\t10000\t1.2962286\t1.4714578e-06\n","\t\t1000\t1.4211725\t9.033162e-05\n","\t\t2000\t1.3722439\t2.0414454e-05\n","\t\t3000\t1.345317\t1.2848366e-05\n","\t\t4000\t1.3317665\t7.966519e-06\n","\t\tFinal Loss:\n","\t\t4411\t1.326409\t6.2911556e-07\n"]}],"source":["n_hidden_units = 3\n","\n","#number of classes\n","C = len(torch.unique(y))\n","\n","model = lambda: torch.nn.Sequential(\n","                    torch.nn.Linear(M, n_hidden_units), #M features to H hiden units\n","                    # 1st transfer function, either Tanh or ReLU:\n","                    #torch.nn.Tanh(),                            \n","                    torch.nn.ReLU(),\n","                    torch.nn.Linear(n_hidden_units, C), # H hidden units to 1 output neuron\n","                    torch.nn.Softmax(dim=1) # final tranfer function\n","                    )\n","\n","loss_fn = torch.nn.CrossEntropyLoss()\n","\n","max_iter = 10000\n","print('Training model of type:\\n{}\\n'.format(str(model())))\n","\n","K = 5\n","CV = model_selection.KFold(n_splits=K,shuffle=True)\n","\n","error_test = np.zeros(K)\n","error_train = np.zeros(K)\n","\n","n_replicates = 3\n","max_iter = 10000\n","tolerance = 1e-6\n","logging_frequency = 1000\n","best_final_loss = 1e100\n","\n","for i, (train_index, test_index) in enumerate(CV.split(X,y)):\n","    X_train, X_test = torch.tensor(X[train_index, :], dtype=torch.float), torch.tensor(X[test_index, :], dtype=torch.float)\n","    y_train, y_test = torch.tensor(y[train_index]) , torch.tensor(y[test_index])\n","\n","    # training\n","\n","    for r in range(n_replicates):\n","        net = model()\n","\n","        torch.nn.init.xavier_uniform_(net[0].weight)\n","        torch.nn.init.xavier_uniform_(net[2].weight)\n","\n","        optimizer = torch.optim.Adam(net.parameters())\n","\n","        learning_curve = []\n","        old_loss = 1e6\n","        for i in range(max_iter):\n","            y_est = net(X_train)\n","            #y_class = torch.max(y_est, dim=1)[1]\n","            loss = loss_fn(y_est, y_train)\n","            loss_value = loss.data.numpy()\n","            learning_curve.append(loss_value)\n","\n","\n","            p_delta_loss = np.abs(loss_value - old_loss)/old_loss\n","            if p_delta_loss < tolerance: break\n","            old_loss = loss_value\n","\n","\n","            if (i != 0) & ((i+1) % logging_frequency == 0):\n","                print_str = '\\t\\t' + str(i+1) + '\\t' + str(loss_value) + '\\t' + str(p_delta_loss)\n","                print(print_str)\n","            optimizer.zero_grad(); loss.backward(); optimizer.step()\n","\n","        \n","\n","        print('\\t\\tFinal Loss:')\n","        print_str = '\\t\\t' + str(i+1) + '\\t' + str(loss_value) + '\\t' + str(p_delta_loss)\n","        print(print_str)\n","\n","        if loss_value < best_final_loss:\n","            best_net = net\n","            best_final_loss = loss_value\n","            best_learning_curve = learning_curve\n"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of miss-classifications for ANN:\n","\t 17 out of 42\n"]},{"name":"stderr","output_type":"stream","text":["/var/folders/k7/4m0jgf1s5rggf6q996sfswf00000gn/T/ipykernel_13379/590878696.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  softmax_logits = net(torch.tensor(X_test, dtype=torch.float))\n"]}],"source":["softmax_logits = net(torch.tensor(X_test, dtype=torch.float))\n","y_test_est = (torch.max(softmax_logits, dim=1)[1]).data.numpy() \n","e = (y_test_est != np.array(y_test))\n","\n","print('Number of miss-classifications for ANN:\\n\\t {0} out of {1}'.format(sum(e),len(e)))\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.2 ('AndSem')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"d62badb4c3b216db4db7804bb7696037312a1bbdb2c7465cfb03cc785a825525"}}},"nbformat":4,"nbformat_minor":2}
